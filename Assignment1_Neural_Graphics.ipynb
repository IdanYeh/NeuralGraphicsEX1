{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR0SKBDbaOqR"
      },
      "source": [
        "# Neural Graphics Ex1: Training Your Own Diffusion Model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryMrLOORbWLz"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lX3XpcGSXBIs"
      },
      "outputs": [],
      "source": [
        "# We recommend using these utils.\n",
        "# https://google.github.io/mediapy/mediapy.html\n",
        "# https://einops.rocks/\n",
        "!pip install mediapy einops --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VdFQ6c9-Pm4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70c24ff-462f-4be9-a7db-067dcc0e7e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import essential modules. Feel free to add whatever you need.\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seed your work\n",
        "To be able to reproduce your code, please use a random seed from this point onward."
      ],
      "metadata": {
        "id": "pDJ3QzHdRV52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "YOUR_SEED = 180 # modify if you want\n",
        "seed_everything(YOUR_SEED)"
      ],
      "metadata": {
        "id": "aDVpoyjaRcoC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dokvxybn_DwK"
      },
      "source": [
        "## 1. Basic Ops and UNet blocks\n",
        "**Notations:**  \n",
        " * `Conv2D(kernel_size, stride, padding)` is `nn.Conv2d()`  \n",
        " * `BN` is `nn.BatchNorm2d()`  \n",
        " * `GELU` is `nn.GELU()`  \n",
        " * `ConvTranspose2D(kernel_size, stride, padding)` is `nn.ConvTranspose2d()`  \n",
        " * `AvgPool(kernel_size)` is `nn.AvgPool2d()`  \n",
        " * `Linear` is `nn.Linear()`  \n",
        " * `N`, `C`, `W` and `H` are batch size, channels num, weight and height respectively\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Ops"
      ],
      "metadata": {
        "id": "k1iFNqV8HjzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional layer that doesn’t change the image\n",
        "    resolution, only the channel dimension\n",
        "    Applies nn.Conv2d(3, 1, 1) followed by BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the Conv layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv =nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(num_features=out_channels),\n",
        "                                nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        return self.conv(x)\n",
        "\n",
        "class DownConv(nn.Module):\n",
        "    \"\"\"\n",
        "        A convolutional layer downsamples the tensor by 2.\n",
        "        The layer consists of Conv2D(3, 2, 1) followed by BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the DownConv layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.downconv =nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=3, stride=2, padding=1),\n",
        "                                nn.BatchNorm2d(num_features=out_channels),\n",
        "                                nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H/2, W/2) output tensor.\n",
        "        \"\"\"\n",
        "        return self.downconv(x)\n",
        "\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional layer that upsamples the tensor by 2.\n",
        "    The layer consists of ConvTranspose2d(4, 2, 1) followed by\n",
        "    BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the UpConv layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.upconv =nn.Sequential(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=4, stride=2, padding=1),\n",
        "                                nn.BatchNorm2d(num_features=out_channels),\n",
        "                                nn.GELU())\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H*2, W*2) output tensor.\n",
        "        \"\"\"\n",
        "        return self.upconv(x)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"\n",
        "    Average pooling layer that flattens a 7x7 tensor into a 1x1 tensor.\n",
        "    The layer consists of AvgPool followed by GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Sequential(nn.AvgPool2d(kernel_size=7),\n",
        "                                     nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, 7, 7) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, C, 1, 1) output tensor.\n",
        "        \"\"\"\n",
        "        return self.flatten(x)\n",
        "\n",
        "\n",
        "class Unflatten(nn.Module):\n",
        "    \"\"\"\n",
        "      Convolutional layer that unflattens/upsamples a 1x1 tensor into a\n",
        "      7x7 tensor. The layer consists of ConvTranspose2D(7, 7, 0)\n",
        "      followed by BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes Unflatten layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.unflatten = nn.Sequential(nn.ConvTranspose2d(in_channels=in_channels,out_channels=in_channels,\n",
        "                                                          kernel_size=7, stride=7,padding=0),\n",
        "                                       nn.BatchNorm2d(num_features=in_channels),\n",
        "                                       nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, 1, 1) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, in_channels, 7, 7) output tensor.\n",
        "        \"\"\"\n",
        "        return self.unflatten(x)\n",
        "\n",
        "class FC(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully connected layer, consisting of nn.linear followed by GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the FC layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc =nn.Sequential(nn.Linear(in_features=in_channels, out_features=out_channels),\n",
        "                               nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels) output tensor.\n",
        "        \"\"\"\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "K4nYHn7gXFWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNet Blocks"
      ],
      "metadata": {
        "id": "GCawfhu0HqcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Two consecutive Conv operations.\n",
        "    Note that it has the same input and output shape as Conv.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes ConvBlock\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.convblock = nn.Sequential(Conv(in_channels, out_channels),\n",
        "                                       Conv(out_channels, out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        return self.convblock(x)\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    DownConv followed by ConvBlock. Note that it has the same input and output\n",
        "    shape as DownConv.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes DownBlock\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.downconv = nn.Sequential(DownConv(in_channels, out_channels),\n",
        "                                      ConvBlock(out_channels, out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H/2, W/2) output tensor.\n",
        "        \"\"\"\n",
        "        return self.downconv(x)\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    UpConv followed by ConvBlock.\n",
        "    Note that it has the same input and output shape as UpConv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes UpBlock\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.upconv = nn.Sequential(UpConv(in_channels, out_channels),\n",
        "                                    ConvBlock(out_channels, out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H*2, W*2) output tensor.\n",
        "        \"\"\"\n",
        "        return self.upconv(x)\n",
        "\n",
        "class FCBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully-connected Block, consisting of FC layer followed by Linear layer. Note\n",
        "    that it has the same input and output shape as FC.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes FCBlock\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fcblock = nn.Sequential(FC(in_channels,out_channels),\n",
        "                                     nn.Linear(out_channels,out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels) output tensor.\n",
        "        \"\"\"\n",
        "        return self.fcblock(x)"
      ],
      "metadata": {
        "id": "3nusVuFlHt67"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMI3IMkjayxQ"
      },
      "source": [
        "## 2. Unconditional Diffusion Framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 UNet architecture"
      ],
      "metadata": {
        "id": "t9JhNQN5Ad3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fkchbyYkzAvV"
      },
      "outputs": [],
      "source": [
        "class DenoisingUNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_hiddens: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ConvIn = ConvBlock(in_channels, num_hiddens)\n",
        "        self.DownBlock1 = DownBlock(num_hiddens, num_hiddens)\n",
        "        self.DownBlock2 = DownBlock(num_hiddens, 2*num_hiddens)\n",
        "        self.Flatten = Flatten()\n",
        "        self.Unflatten = Unflatten(2*num_hiddens)\n",
        "        self.UpBlock1 = UpBlock(4*num_hiddens, num_hiddens)\n",
        "        self.UpBlock2 = UpBlock(2*num_hiddens, num_hiddens)\n",
        "        self.ConvOut = ConvBlock(2*num_hiddens, num_hiddens)\n",
        "        self.Conv2D = nn.Conv2d(num_hiddens, in_channels, kernel_size=(3,3), padding=(1,1))\n",
        "\n",
        "        # time\n",
        "        self.FcBlock1 = FCBlock(1, 2*num_hiddens)\n",
        "        self.FcBlock2 = FCBlock(1, num_hiddens)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        t: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "            t: (N, 1) normalized time tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, C, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
        "\n",
        "        # Down\n",
        "\n",
        "        res_high = self.ConvIn(x)\n",
        "        res_mid = self.DownBlock1(res_high)\n",
        "        res_low = self.DownBlock2(res_mid)\n",
        "        flattened = self.Flatten(res_low)\n",
        "\n",
        "        # time in\n",
        "\n",
        "        t_low = self.FcBlock1(t).unsqueeze(-1).unsqueeze(-1)\n",
        "        t_mid = self.FcBlock2(t).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Up\n",
        "\n",
        "        out = self.Unflatten(flattened) + t_low\n",
        "        out = self.UpBlock1(torch.cat((res_low, out), dim=1)) + t_mid\n",
        "        out = self.UpBlock2(torch.cat((res_mid, out), dim=1))\n",
        "        out = self.Conv2D(self.ConvOut(torch.cat((res_high, out), dim=1)))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fhpEzgwCJqbW"
      },
      "outputs": [],
      "source": [
        "class Conv(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional layer that doesn’t change the image\n",
        "    resolution, only the channel dimension\n",
        "    Applies nn.Conv2d(3, 1, 1) followed by BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the Conv layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv =nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=3, stride=1, padding=1),\n",
        "                                nn.BatchNorm2d(num_features=out_channels),\n",
        "                                nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        return self.conv(x)\n",
        "\n",
        "class DownConv(nn.Module):\n",
        "    \"\"\"\n",
        "        A convolutional layer downsamples the tensor by 2.\n",
        "        The layer consists of Conv2D(3, 2, 1) followed by BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the DownConv layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.downconv =nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=3, stride=2, padding=1),\n",
        "                                nn.BatchNorm2d(num_features=out_channels),\n",
        "                                nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H/2, W/2) output tensor.\n",
        "        \"\"\"\n",
        "        return self.downconv(x)\n",
        "\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional layer that upsamples the tensor by 2.\n",
        "    The layer consists of ConvTranspose2d(4, 2, 1) followed by\n",
        "    BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the UpConv layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.upconv =nn.Sequential(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                kernel_size=4, stride=2, padding=1),\n",
        "                                nn.BatchNorm2d(num_features=out_channels),\n",
        "                                nn.GELU())\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels, H*2, W*2) output tensor.\n",
        "        \"\"\"\n",
        "        return self.upconv(x)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"\n",
        "    Average pooling layer that flattens a 7x7 tensor into a 1x1 tensor.\n",
        "    The layer consists of AvgPool followed by GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Sequential(nn.AvgPool2d(kernel_size=7),\n",
        "                                     nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, 7, 7) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, C, 1, 1) output tensor.\n",
        "        \"\"\"\n",
        "        return self.flatten(x)\n",
        "\n",
        "\n",
        "class Unflatten(nn.Module):\n",
        "    \"\"\"\n",
        "      Convolutional layer that unflattens/upsamples a 1x1 tensor into a\n",
        "      7x7 tensor. The layer consists of ConvTranspose2D(7, 7, 0)\n",
        "      followed by BN and GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes Unflatten layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.unflatten = nn.Sequential(nn.ConvTranspose2d(in_channels=in_channels,out_channels=in_channels,\n",
        "                                                          kernel_size=7, stride=7,padding=0),\n",
        "                                       nn.BatchNorm2d(num_features=in_channels),\n",
        "                                       nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels, 1, 1) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, in_channels, 7, 7) output tensor.\n",
        "        \"\"\"\n",
        "        return self.unflatten(x)\n",
        "\n",
        "class FC(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully connected layer, consisting of nn.linear followed by GELU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the FC layer\n",
        "        Args:\n",
        "            in_channels (int): The number of input channels\n",
        "            out_channels (int): The number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc =nn.Sequential(nn.Linear(in_features=in_channels, out_features=out_channels),\n",
        "                               nn.GELU())\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, in_channels) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, out_channels) output tensor.\n",
        "        \"\"\"\n",
        "        return self.fc(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nyxOM-RbZnC"
      },
      "source": [
        "### 2.1 DDPM Forward and Inverse Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yIvMw63T6JkE"
      },
      "outputs": [],
      "source": [
        "def ddpm_schedule(beta1: float, beta2: float, num_ts: int, device: str = 'cuda') -> dict:\n",
        "    \"\"\"Constants for DDPM training and sampling.\n",
        "\n",
        "    Arguments:\n",
        "        beta1: float, starting beta value.\n",
        "        beta2: float, ending beta value.\n",
        "        num_ts: int, number of timesteps.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys:\n",
        "            betas: linear schedule of betas from beta1 to beta2.\n",
        "            alphas: 1 - betas.\n",
        "            alpha_bars: cumulative product of alphas.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"Expect beta1 < beta2 < 1.0.\"\n",
        "\n",
        "    betas = torch.linspace(beta1, beta2, num_ts).to(device)\n",
        "    alphas = 1 - betas\n",
        "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    out_dict = {\n",
        "        'betas': betas,\n",
        "        'alphas': alphas,\n",
        "        'alpha_bars': alpha_bars,\n",
        "    }\n",
        "    return out_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hfvtHEFf_7Q3"
      },
      "outputs": [],
      "source": [
        "def ddpm_forward(\n",
        "    unet: DenoisingUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    x_0: torch.Tensor,\n",
        "    num_ts: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 1 of the DDPM paper (not including gradient step).\n",
        "\n",
        "    Args:\n",
        "        unet: DenoisingUNet\n",
        "        ddpm_schedule: dict\n",
        "        x_0: (N, C, H, W) input tensor.\n",
        "        num_ts: int, number of timesteps.\n",
        "    Returns:\n",
        "        (,) diffusion loss.\n",
        "    \"\"\"\n",
        "    unet.train()\n",
        "    # YOUR CODE HERE.\n",
        "    device = x_0.device\n",
        "    N = x_0.shape[0]\n",
        "    ts = torch.randint(1, num_ts+1, (N,), device=device)\n",
        "    epsilons = torch.randn_like(x_0, device=device)\n",
        "    alpha_bars = ddpm_schedule['alpha_bars'][ts-1]\n",
        "    alpha_bars = alpha_bars.view(-1, 1, 1, 1)\n",
        "    xt_vec = torch.sqrt(alpha_bars)*x_0 + torch.sqrt(1-alpha_bars)*epsilons\n",
        "    ts_normalized = (ts/num_ts).view(-1,1)\n",
        "    epsilons_est = unet(xt_vec,ts_normalized)\n",
        "    loss = nn.functional.mse_loss(epsilons, epsilons_est)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BNE8-455IDm3"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def ddpm_sample(\n",
        "    unet: DenoisingUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    img_wh: tuple[int, int],\n",
        "    batch_size: int,\n",
        "    num_ts: int\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 2 of the DDPM paper.\n",
        "\n",
        "    Args:\n",
        "        unet: DenoisingUNet\n",
        "        ddpm_schedule: dict\n",
        "        img_wh: (H, W) output image width and height.\n",
        "        num_ts: int, number of timesteps.\n",
        "\n",
        "    Returns:\n",
        "        (N, C, H, W) final sample.\n",
        "    \"\"\"\n",
        "    unet.eval()\n",
        "    # YOUR CODE HERE.\n",
        "    device = next(unet.parameters()).device\n",
        "    H, W = img_wh\n",
        "    C = unet.in_channels\n",
        "    xt = torch.randn(batch_size, C, H, W, device=device)\n",
        "\n",
        "    alpha_bars = ddpm_schedule['alpha_bars']\n",
        "    beta = ddpm_schedule['beta']\n",
        "\n",
        "    for t in range(num_ts,0,-1):\n",
        "        z = torch.randn_like(xt) if t > 1 else torch.zeros_like(xt)\n",
        "        at = alpha_bars[t-1].view(-1, 1, 1, 1)\n",
        "\n",
        "        t_norm = torch.full((batch_size, 1), t / num_ts, device=device)\n",
        "        epsilons_est = unet(xt,t_norm)\n",
        "\n",
        "        x0_est = (1/torch.sqrt(at))*(xt-torch.sqrt(1-at)*epsilons_est)\n",
        "\n",
        "        at_next = alpha_bars[t-2].view(-1, 1, 1, 1) if t > 1 else torch.ones_like(at)\n",
        "        bt = beta[t-1].view(-1, 1, 1, 1)\n",
        "\n",
        "        xt = (torch.sqrt(at_next)*bt/(1-at))*x0_est + (torch.sqrt(at)*(1-at_next)/(1-at))*xt + torch.sqrt(bt)*z\n",
        "\n",
        "    return xt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G_hVifFyw20j"
      },
      "outputs": [],
      "source": [
        "# Do Not Modify\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unet: DenoisingUNet,\n",
        "        betas: tuple[float, float] = (1e-4, 0.02),\n",
        "        num_ts: int = 300,\n",
        "        p_uncond: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.unet = unet\n",
        "        self.betas = betas\n",
        "        self.num_ts = num_ts\n",
        "        self.p_uncond = p_uncond\n",
        "        self.ddpm_schedule = ddpm_schedule(betas[0], betas[1], num_ts)\n",
        "\n",
        "        for k, v in ddpm_schedule(betas[0], betas[1], num_ts).items():\n",
        "            self.register_buffer(k, v, persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (,) diffusion loss.\n",
        "        \"\"\"\n",
        "        return ddpm_forward(\n",
        "            self.unet, self.ddpm_schedule, x, self.num_ts\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def sample(\n",
        "        self,\n",
        "        img_wh: tuple[int, int],\n",
        "        batch_size: int\n",
        "    ):\n",
        "        return ddpm_sample(\n",
        "            self.unet, self.ddpm_schedule, img_wh, batch_size, self.num_ts\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Train your denoiser"
      ],
      "metadata": {
        "id": "ACe_cr2_dv7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, scheduler, batch_losses, epoch_losses, CHECKPOINT_DIR):\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'batch_losses': batch_losses,\n",
        "        'epoch_losses': epoch_losses,\n",
        "    },  os.path.join(CHECKPOINT_DIR, f'ddpm_epoch_{epoch}.pth'))\n",
        "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "def load_checkpoint(CHECKPOINT_DIR, train_from_scratch, num_hidden, lr, num_epochs, clear_folder):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Init denoiser and DDPM wrapper\n",
        "    denosier_unet = DenoisingUNet(in_channels=1 , num_hiddens=num_hidden)\n",
        "    ddpm = DDPM(denosier_unet, num_ts=T)\n",
        "\n",
        "    # Optimizer and device setup - Adam optimizer with exponential learning rate decay\n",
        "    optimizer = torch.optim.Adam(ddpm.parameters(), lr = lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = 0.1**(1.0/num_epochs))\n",
        "    ddpm.to(device)\n",
        "\n",
        "    # Create/Clear folder if necessary\n",
        "    if clear_folder and os.path.exists(CHECKPOINT_DIR):\n",
        "        print(f\"Clearing all checkpoints in: {CHECKPOINT_DIR}\")\n",
        "        # Delete the whole directory and recreate it\n",
        "        shutil.rmtree(CHECKPOINT_DIR)\n",
        "    if not os.path.exists(CHECKPOINT_DIR):\n",
        "        os.makedirs(CHECKPOINT_DIR)\n",
        "        print(f\"Created directory: {CHECKPOINT_DIR}\")\n",
        "\n",
        "    # Logic to find the latest checkpoint and load it\n",
        "    latest_checkpoint = None\n",
        "\n",
        "    # Check for existing files in the directory\n",
        "    if os.path.exists(CHECKPOINT_DIR):\n",
        "        files = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pth')]\n",
        "        if files:\n",
        "            # Sort by epoch number (assumes name format 'ddpm_epoch_X.pth')\n",
        "            files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "            latest_checkpoint = os.path.join(CHECKPOINT_DIR, files[-1])\n",
        "\n",
        "    if (not latest_checkpoint is None) and (not train_from_scratch):\n",
        "        print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
        "        checkpoint = torch.load(latest_checkpoint)\n",
        "\n",
        "        ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        batch_losses = checkpoint.get('batch_losses', [])\n",
        "        epoch_losses = checkpoint.get('epoch_losses', [])\n",
        "        print(f\"Resuming from epoch {start_epoch}\")\n",
        "\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        batch_losses = []\n",
        "        epoch_losses = []\n",
        "        print(f\"training from scratch\")\n",
        "\n",
        "    if start_epoch > num_epochs:\n",
        "      raise IOError(f'The loaded model epoch was {start_epoch}, while the request number of epochs is {num_epochs}.')\n",
        "\n",
        "    return start_epoch, ddpm, optimizer, scheduler, batch_losses, epoch_losses"
      ],
      "metadata": {
        "id": "mW5fbJlMHczH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# eval setup\n",
        "CHECKPOINT_DIR = \"/content/drive/NeuralGraphicsEX1/ddpm_checkpoints\"\n",
        "train_from_scratch = False\n",
        "clear_folder = False\n",
        "eval_epochs = [1, 2, 3, 5, 10, 15, 20]\n",
        "\n",
        "# Hyper parameters - Modify if you wish\n",
        "num_hidden = 32 # 128\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "lr = 1e-3\n",
        "img_wh = (28, 28)\n",
        "eval_batch_size=20\n",
        "T=300\n",
        "\n",
        "# initial\\load checkpoint\n",
        "start_epoch, ddpm, optimizer, scheduler, batch_losses, epoch_losses = load_checkpoint(CHECKPOINT_DIR, train_from_scratch, num_hidden, lr, num_epochs, clear_folder)\n",
        "\n",
        "# Init MNIST data loaders\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_data = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = DataLoader(test_data, batch_size=eval_batch_size, shuffle=True) #Not usefull now, but will be for evaluating class-conditioned denoiser (3.3)\n",
        "\n",
        "for epoch in tqdm(range(start_epoch, num_epochs)):\n",
        "  ddpm.train()  # Set the model to training mode\n",
        "  epoch_loss = 0.0\n",
        "  for batch, (data, label) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      data = data.to(device)\n",
        "      loss = ddpm(data)\n",
        "      loss.backward()  # Compute gradients\n",
        "      optimizer.step()  # Update weights\n",
        "      batch_loss = loss.item()\n",
        "      epoch_loss += batch_loss\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch}/{len(train_loader)}], Loss: {batch_loss:.4f}\")\n",
        "        batch_losses.append(batch_loss)\n",
        "\n",
        "  avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "  epoch_losses.append(epoch_loss)\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_epoch_loss:.4f}\")\n",
        "  scheduler.step()\n",
        "\n",
        "  ddpm.eval() # changes the behaior of BN and Dropouts layers\n",
        "  # YOUR EVAL CODE HERE.\n",
        "  with torch.no_grad():\n",
        "\n",
        "      if (epoch + 1) in eval_epochs:\n",
        "          print(f\"Generating samples for epoch {epoch + 1}...\")\n",
        "\n",
        "          samples = ddpm.sample(img_wh=img_wh, batch_size=eval_batch_size)\n",
        "\n",
        "          grid = vutils.make_grid(samples, nrow=int(eval_batch_size // 4), normalize=True)\n",
        "          plt.figure(figsize=(10, 10))\n",
        "          plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "          plt.title(f\"Samples after {epoch + 1} epochs\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "          # Recommended: Checkpoint the model as suggested in the tips\n",
        "\n",
        "  save_checkpoint(epoch+1, ddpm, optimizer, scheduler, batch_losses, epoch_losses, CHECKPOINT_DIR)\n",
        "\n",
        "# Final Plotting Logic\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Batch Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(batch_losses, label='Batch Loss')\n",
        "plt.title('Training Batch MSE Loss')\n",
        "plt.xlabel('Iterations (every 100 steps)')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Epoch Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, label='Epoch Loss', color='orange')\n",
        "plt.title('Training Epoch Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total MSE Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')"
      ],
      "metadata": {
        "id": "VSChVRmJYO7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "ba146764-8310-46b6-c431-6714b87d1015"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'ConvTranspose2D'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3294504723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# initial\\load checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_from_scratch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Init MNIST data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1942573796.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(CHECKPOINT_DIR, train_from_scratch, num_hidden, lr, num_epochs, clear_folder)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Init denoiser and DDPM wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdenosier_unet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenoisingUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mddpm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenosier_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2066239627.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, num_hiddens)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownBlock2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpBlock1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpBlock2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2471020155.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \"\"\"\n\u001b[1;32m    123\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         self.unflatten = nn.Sequential(nn.ConvTranspose2D(in_channels=in_channels,out_channels=in_channels,\n\u001b[0m\u001b[1;32m    125\u001b[0m                                                           kernel_size=7, stride=7,padding=0),\n\u001b[1;32m    126\u001b[0m                                        \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'ConvTranspose2D'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementing class-conditioned diffusion framework with CFG\n"
      ],
      "metadata": {
        "id": "uW2FBjpn8CTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 Adding Class-Conditioning to UNet architecture"
      ],
      "metadata": {
        "id": "irot7PI1I2Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDenoisingUNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        num_hiddens: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE.\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        c: torch.Tensor,\n",
        "        t: torch.Tensor,\n",
        "        mask: torch.Tensor | None = None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "            c: (N, num_classes) float condition tensor.\n",
        "            t: (N, 1) normalized time tensor.\n",
        "            mask: (N, 1) mask tensor. If not None, mask out condition when mask == 0.\n",
        "\n",
        "        Returns:\n",
        "            (N, C, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
        "        # YOUR CODE HERE.\n",
        "        raise NotImplementedError()"
      ],
      "metadata": {
        "id": "vAXZYlOt8Rzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 DDPM Forward and Inverse Process with CFG"
      ],
      "metadata": {
        "id": "uV3lTJz8IxrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ddpm_forward(\n",
        "    unet: ConditionalDenoisingUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    x_0: torch.Tensor,\n",
        "    c: torch.Tensor,\n",
        "    p_uncond: float,\n",
        "    num_ts: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 3 (not including gradient step).\n",
        "\n",
        "    Args:\n",
        "        unet: ConditionalDenoisingUNet\n",
        "        ddpm_schedule: dict\n",
        "        x_0: (N, C, H, W) input tensor.\n",
        "        c: (N,) int64 condition tensor.\n",
        "        p_uncond: float, probability of unconditioning the condition.\n",
        "        num_ts: int, number of timesteps.\n",
        "\n",
        "    Returns:\n",
        "        (,) diffusion loss.\n",
        "    \"\"\"\n",
        "    unet.train()\n",
        "    # YOUR CODE HERE.\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "NobmVh4U8BRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def ddpm_cfg_sample(\n",
        "    unet: ConditionalDenoisingUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    c: torch.Tensor,\n",
        "    img_wh: tuple[int, int],\n",
        "    num_ts: int,\n",
        "    guidance_scale: float = 5.0\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 4.\n",
        "\n",
        "    Args:\n",
        "        unet: ConditionalDenoisingUNet\n",
        "        ddpm_schedule: dict\n",
        "        c: (N,) int64 condition tensor. Only for class-conditional\n",
        "        img_wh: (H, W) output image width and height.\n",
        "        num_ts: int, number of timesteps.\n",
        "        guidance_scale: float, CFG scale.\n",
        "\n",
        "    Returns:\n",
        "        (N, C, H, W) final sample.\n",
        "    \"\"\"\n",
        "    unet.eval()\n",
        "    # YOUR CODE HERE.\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "rMW5YeCi8cqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Not Modify\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unet: ConditionalDenoisingUNet,\n",
        "        betas: tuple[float, float] = (1e-4, 0.02),\n",
        "        num_ts: int = 300,\n",
        "        p_uncond: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.unet = unet\n",
        "        self.betas = betas\n",
        "        self.num_ts = num_ts\n",
        "        self.p_uncond = p_uncond\n",
        "        self.ddpm_schedule = ddpm_schedule(betas[0], betas[1], num_ts)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "            c: (N,) int64 condition tensor.\n",
        "\n",
        "        Returns:\n",
        "            (,) diffusion loss.\n",
        "        \"\"\"\n",
        "        return ddpm_forward(\n",
        "            self.unet, self.ddpm_schedule, x, c, self.p_uncond, self.num_ts\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def sample(\n",
        "        self,\n",
        "        c: torch.Tensor,\n",
        "        img_wh: tuple[int, int],\n",
        "        guidance_scale: float = 5.0\n",
        "    ):\n",
        "        return ddpm_cfg_sample(\n",
        "            self.unet, self.ddpm_schedule, c, img_wh, self.num_ts, guidance_scale\n",
        "        )"
      ],
      "metadata": {
        "id": "gdQFWwIt8mXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3 Train your class-conditioned denoiser"
      ],
      "metadata": {
        "id": "EEGqlFNClOaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE."
      ],
      "metadata": {
        "id": "MkAIikcEMFEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4 Experiment with different guidance sacles"
      ],
      "metadata": {
        "id": "m4iTw-TFGYhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE.\n"
      ],
      "metadata": {
        "id": "d9gnGqPOoXT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ryMrLOORbWLz",
        "k1iFNqV8HjzM",
        "GCawfhu0HqcH",
        "t9JhNQN5Ad3V",
        "0nyxOM-RbZnC",
        "m4iTw-TFGYhA"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "cs180-proj5",
      "language": "python",
      "name": "cs180-proj5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}